{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/paroma/anaconda2/envs/babble/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/dfs/scratch0/paroma/anaconda2/envs/babble/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/dfs/scratch0/paroma/anaconda2/envs/babble/lib/python2.7/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "\n",
    "#from imdb_lstm import *\n",
    "from imdb_glove import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.load('/dfs/scratch0/paroma/data/imdb/lstm_train_ground.npy')\n",
    "y_train = np.load('/dfs/scratch0/paroma/reef/imdb_gt.npy')\n",
    "\n",
    "train_text = np.load('/dfs/scratch0/paroma/data/imdb/lstm_train_text.npy')\n",
    "test_text = np.load('/dfs/scratch0/paroma/data/imdb/lstm_val_text.npy')\n",
    "y_test = np.load('/dfs/scratch0/paroma/data/imdb/lstm_val_ground.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "WARNING:tensorflow:From /dfs/scratch0/paroma/anaconda2/envs/babble/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /dfs/scratch0/paroma/anaconda2/envs/babble/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          394000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 474,501\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 394,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 284 samples, validate on 284 samples\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 5s 16ms/step - loss: 0.7082 - acc: 0.5106 - val_loss: 0.7020 - val_acc: 0.5458\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 5s 16ms/step - loss: 0.6766 - acc: 0.5704 - val_loss: 0.6887 - val_acc: 0.5317\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 5s 17ms/step - loss: 0.6583 - acc: 0.6232 - val_loss: 0.6922 - val_acc: 0.5282\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 5s 19ms/step - loss: 0.6452 - acc: 0.6972 - val_loss: 0.6978 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 5s 17ms/step - loss: 0.6278 - acc: 0.6761 - val_loss: 0.7067 - val_acc: 0.5423\n",
      "Accuracy: 54.23%\n",
      "Found 400000 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          389400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 469,901\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 389,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 284 samples, validate on 284 samples\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 5s 18ms/step - loss: 0.7093 - acc: 0.4754 - val_loss: 0.7166 - val_acc: 0.5458\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 5s 17ms/step - loss: 0.7115 - acc: 0.5317 - val_loss: 0.6876 - val_acc: 0.5599\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 5s 17ms/step - loss: 0.6794 - acc: 0.5704 - val_loss: 0.6811 - val_acc: 0.5634\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 5s 17ms/step - loss: 0.6700 - acc: 0.6655 - val_loss: 0.6751 - val_acc: 0.5880\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 5s 17ms/step - loss: 0.6605 - acc: 0.6444 - val_loss: 0.6672 - val_acc: 0.6197\n",
      "Accuracy: 61.97%\n",
      "Found 400000 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 100)          394100    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 474,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 394,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 284 samples, validate on 284 samples\n",
      "Epoch 1/5\n",
      "284/284 [==============================] - 6s 22ms/step - loss: 0.7117 - acc: 0.4894 - val_loss: 0.6656 - val_acc: 0.5775\n",
      "Epoch 2/5\n",
      "284/284 [==============================] - 5s 19ms/step - loss: 0.6888 - acc: 0.5282 - val_loss: 0.6636 - val_acc: 0.5810\n",
      "Epoch 3/5\n",
      "284/284 [==============================] - 5s 19ms/step - loss: 0.6761 - acc: 0.5599 - val_loss: 0.6638 - val_acc: 0.6444\n",
      "Epoch 4/5\n",
      "284/284 [==============================] - 5s 19ms/step - loss: 0.6726 - acc: 0.6056 - val_loss: 0.6755 - val_acc: 0.5563\n",
      "Epoch 5/5\n",
      "284/284 [==============================] - 5s 18ms/step - loss: 0.6781 - acc: 0.5317 - val_loss: 0.6740 - val_acc: 0.5739\n",
      "Accuracy: 57.39%\n"
     ]
    }
   ],
   "source": [
    "f1_all = []\n",
    "pr_all = []\n",
    "re_all = []\n",
    "val_acc_all = []\n",
    "\n",
    "\n",
    "bs_arr = [64,128,256]\n",
    "#n_epochs_arr = [5,10,25]\n",
    "n_epochs_arr = [5]\n",
    "\n",
    "for bs in bs_arr:\n",
    "    for n in n_epochs_arr:\n",
    "        y_pred = lstm_glove(train_text, y_train, test_text, y_test, bs=bs, n=n)\n",
    "        #predictions = np.round(y_pred)\n",
    "        predictions = np.abs(1-np.round(y_pred))\n",
    "        \n",
    "        val_acc_all.append(np.sum(predictions == y_test)/float(np.shape(y_test)[0]))\n",
    "        f1_all.append(metrics.f1_score(y_test, predictions))\n",
    "        pr_all.append(metrics.precision_score(y_test, predictions))\n",
    "        re_all.append(metrics.recall_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Batch Size:  64\n",
      "Best Epochs:  5\n",
      "F1 Score:  0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "ii,jj = np.unravel_index(np.argmax(f1_all), (3,3))\n",
    "print 'Best Batch Size: ', bs_arr[ii]\n",
    "print 'Best Epochs: ', n_epochs_arr[jj]\n",
    "print 'F1 Score: ', max(f1_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Val Acc:  0.45774647887323944\n",
      "\n",
      "Best F1:  0.6111111111111112\n",
      "Best Pr:  0.45318352059925093\n",
      "Best Re:  0.937984496124031\n"
     ]
    }
   ],
   "source": [
    "print 'Best Val Acc: ', np.max(val_acc_all)\n",
    "print '\\nBest F1: ', np.max(f1_all)\n",
    "print 'Best Pr: ', pr_all[np.argmax(f1_all)]\n",
    "print 'Best Re: ', re_all[np.argmax(f1_all)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
